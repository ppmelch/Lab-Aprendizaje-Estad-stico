{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Cross Validación\n",
    "La validación cruzada es una técnica para evaluar la capacidad predictiva de un modelo dividiendo los datos en partes (folds). Los tipos más comunes son:\n",
    "\n",
    "- **K-Fold Cross Validation**: Se divide el dataset en \\( k \\) partes, entrenando en \\( k-1 \\) y probando en la restante, repitiendo \\( k \\) veces.\n",
    "- **Leave-One-Out (LOO)**: Cada observación es usada como test, y el resto como entrenamiento.\n",
    "- **Stratified K-Fold**: Como K-Fold, pero mantiene la proporción de clases en clasificación.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Descenso en Gradiente\n",
    "Es un algoritmo de optimización iterativa usado para minimizar funciones de costo en modelos de Machine Learning. La idea es ajustar los parámetros moviéndose en la dirección opuesta al gradiente de la función de pérdida.\n",
    "\n",
    " Variantes:\n",
    "\n",
    "- **Batch Gradient Descent**: Usa todo el dataset en cada iteración.\n",
    "- **Stochastic Gradient Descent (SGD)**: Usa una muestra aleatoria por iteración, reduciendo el costo computacional.\n",
    "- **Mini-batch Gradient Descent**: Usa pequeños subconjuntos de datos, combinando batch y SGD.\n",
    "\n",
    "### ¿Qué es Descenso Gradiente? \n",
    "El descenso en gradiente es un algoritmo que se utiliza para minimizar una función de costo, donde se ajustan los parámetros del modelo de manera iterativa(que se repite). Su objetivo es encontrar los valores óptimos de los parámetros moviéndose en la dirección opuesta al gradiente de la función de pérdida.\n",
    "\n",
    "Los parámetros $\\theta_0, \\theta_1, \\theta_2$ se actualizan iterativamente según la regla del descenso por gradiente:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$\n",
    "\n",
    "Donde:\n",
    "- $\\alpha$ es la tasa de aprendizaje, que determina el tamaño de los pasos.\n",
    "- $(j \\in \\{0, 1, 2\\}$) indica cada parámetro.\n",
    "\n",
    "De forma explícita, las actualizaciones son:\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) $\n",
    "\n",
    "$\\theta_1 := \\theta_1 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x^{(i)}$\n",
    "\n",
    "$\\theta_2 := \\theta_2 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) (x^{(i)})^2$\n",
    "\n",
    "\n",
    "## Iteración hasta convergencia\n",
    "\n",
    "El proceso completo se puede resumir en los siguientes pasos:\n",
    "\n",
    "1. Inicializar los parámetros $\\theta_0$, $\\theta_1$, $\\theta_2$ con valores aleatorios o ceros.\n",
    "2. Calcular las predicciones $\\hat{y}^{(i)}$ para todos los datos.\n",
    "3. Calcular las derivadas parciales $\\frac{\\partial J}{\\partial \\theta_j}$.\n",
    "4. Actualizar los parámetros $\\theta_0$, $\\theta_1$, $\\theta_2$) usando las reglas del descenso por gradiente.\n",
    "5. Repetir hasta que \\(J\\) converja.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Regresiones Lineales Simples\n",
    "Modelo que ajusta una línea recta:\n",
    "$ y = \\beta_0 + \\beta_1 x + \\epsilon $\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ \\beta_0 $ es la intersección.\n",
    "- $ \\beta_1 $ es la pendiente.\n",
    "- $ \\epsilon $ es el error aleatorio.\n",
    "\n",
    "Se entrena minimizando la suma de los cuadrados de los residuos:\n",
    "$ \\sum (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Regresiones Polinomiales\n",
    "Extensión de la regresión lineal, pero usando términos de mayor grado:\n",
    "$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_n x^n + \\epsilon $\n",
    "\n",
    "Aumenta la capacidad del modelo para capturar relaciones no lineales, pero puede llevar a overfitting si el grado es muy alto.\n",
    "\n",
    "La regresión polinómica se utiliza para modelar relaciones no lineales entre la variable independiente \\(x\\) y la variable dependiente \\(y\\). \n",
    "\n",
    "#### Polinomio de grado 2:\n",
    "\n",
    "$\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2$\n",
    "\n",
    "Donde:\n",
    "- $(\\hat{y})$ es la predicción.\n",
    "- $(\\theta_0, \\theta_1, \\theta_2)$ son los parámetros del modelo.\n",
    "- $(x$ es la característica de entrada.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Categorizar Variables\n",
    "En Machine Learning, las variables categóricas deben convertirse a numéricas para ser usadas en modelos. Técnicas comunes:\n",
    "\n",
    "- **One-Hot Encoding**: Convierte cada categoría en una columna binaria (0 o 1).\n",
    "- **Label Encoding**: Asigna números enteros a cada categoría, pero introduce un orden artificial.\n",
    "- **Target Encoding**: Sustituye la categoría por la media de la variable objetivo (riesgo de overfitting).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Prueba de Hipótesis\n",
    "Proceso estadístico para tomar decisiones sobre una población basándose en una muestra.\n",
    "\n",
    "Pasos típicos:\n",
    "1. Formular hipótesis: $ H_0 $ (hipótesis nula) y $ H_A $ (hipótesis alternativa).\n",
    "2. Elegir nivel de significancia ($ \\alpha $, comúnmente 0.05).\n",
    "3. Elegir estadístico de prueba (t-student, chi-cuadrado, etc.).\n",
    "4. Calcular p-valor y comparar con $ \\alpha $.\n",
    "5. Tomar decisión: si $ p $-valor < $ \\alpha $, se rechaza $ H_0 $.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Análisis Bigarrados\n",
    "Se refiere a la exploración de relaciones entre dos variables cuantitativas mediante medidas como la correlación de Pearson y la covarianza.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Teorema de Frisch-Waugh-Lovell\n",
    "Dice que en un modelo de regresión múltiple, el coeficiente estimado de una variable es el mismo que se obtendría al ajustar los residuos de las demás variables y luego hacer una regresión sobre esos residuos. Es útil para entender cómo aislar el efecto de una variable en presencia de otras.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. R² Score (Coeficiente de Determinación)\n",
    "Mide qué tan bien un modelo explica la variabilidad de la variable objetivo. Se calcula como:\n",
    "$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $ SS_{res} $ es la suma de los cuadrados de los residuos.\n",
    "- $ SS_{tot} $ es la suma de los cuadrados totales.\n",
    "\n",
    "Valores cercanos a 1 indican buen ajuste, mientras que valores cercanos a 0 sugieren que el modelo no explica bien la variabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. KNN\n",
    "Es un algoritmo de aprendizaje supervisado usado para clasificación y regresión. Su principio es simple: predice un valor o categoría basado en los K vecinos más cercanos en el espacio de características.\n",
    "\n",
    "\n",
    "1- Se elige un número  K de vecinos (hiperparámetro).\n",
    "\n",
    "2- Para una nueva observación, se calculan las distancias con todos los datos de entrenamiento.\n",
    "\n",
    "3- Se seleccionan los  K vecinos más cercanos. \n",
    "\n",
    "4- Se predice el promedio de los valores de los K vecinos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
