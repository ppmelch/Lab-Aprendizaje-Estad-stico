{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d323cad6-0afa-4c28-9ce0-07c970efdb6b",
   "metadata": {},
   "source": [
    "# Examen – Modelos de Ensamble: Árboles y Boosting\n",
    "\n",
    "**Instrucciones**: Justifica cada respuesta de manera clara. Usa fórmulas donde aplique y sé específico en tus argumentos.\n",
    "\n",
    "\n",
    "José Armando Melchor Soto\n",
    "\n",
    "---\n",
    "\n",
    "## Sección I: Árboles de decisión (15 puntos)\n",
    "\n",
    "### 1. (5 pts)  \n",
    "Explica cómo se construye un árbol y qué criterio usa para decidir los splits. Explica tanto para el caso de clasificación como de regresión.\n",
    "\n",
    "Un árbol de decisión es un modelo predictivo que organiza los datos en distintos niveles jerárquicos, realizando divisiones sucesivas según ciertos valores límite. En cada nivel, separa los datos dependiendo de si cumplen o no con una condición específica, agrupándolos según esas decisiones para llegar a una predicción final.\n",
    "\n",
    "\n",
    "#### Regresión\n",
    "Este modelo utiliza todas las varibles y splits posibles, donde finalmente aplica el tipo de split con mayor reducción de varianza, con base en el error cuadrático medio.\n",
    "$$\n",
    "\\text{Reducción de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "#### Clasificación\n",
    "El clasificador considera todas las variables y posibles puntos de corte, pero en este caso selecciona aquel que mejor separa las clases, buscando el umbral que maximice la ganancia de información.\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de impureza} = \\text{Impureza nodo padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### 2. (5 pts)  \n",
    "Da un ejemplo de sobreajuste en un árbol de decisión. Explica cómo se podría evitar sin necesidad de usar ensambles.\n",
    "\n",
    "Un árbol de decisión sobreajusta cuando crece demasiado y memoriza los datos de entrenamiento. Si se limita la profundidad max_depth, establecer mínimo de muestras por hoja min_samples_leaf, se podría evitar la neceidad de usar ensambles\n",
    "\n",
    "\n",
    "\n",
    "### 3. (5 pts)  \n",
    "Si te fijas, en clase nunca hicimos escalamiento (`StandardScaler`). ¿Por qué los árboles no lo necesitan?\n",
    "\n",
    "Los árboles no requieren escalamiento porque no dependen de distancias entre puntos, sino que dividen los datos con base en umbrales. Por lo tanto no es necesario y por eso nunca lo hicimos.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección II: Random Forest (20 puntos)\n",
    "\n",
    "### 4. (10 pts)  \n",
    "Explica cómo funciona un Random Forest. ¿En qué se basa? ¿Por qué es una buena idea?\n",
    "\n",
    "\n",
    "Es un algoritmo de machine learning basado en árboles de decisión, que crea muchos árboles y combina sus predicciones para mejorar la precisión y evitar el sobreajuste.\n",
    "\n",
    "#### Regresión \n",
    "Se generan varias muestras aleatorias del dataset original usando bootstrap. A partir de esas muestras, se entrenan múltiples árboles. Luego, cada uno hace su predicción y, al final, se toma el promedio de todas las predicciones para obtener el resultado final del modelo.\n",
    "#### Clasificación\n",
    "Se entrenan varios árboles con diferentes muestras de datos. Pero en lugar de promediar, se toma la clase que más veces fue predicha por los árboles, es decir, se elige la modal entre todas las predicciones\n",
    "\n",
    "Esto es buena idea porque reduce la varianza de los arboles residuales y esto mejora la generalización, y tambien se pede decir que es más dificil que tenga overfitting y con eso sigue teniendo un muy buen rendimiento. No se necesitan ajustar mucho los parametros para obtener un muy buen resultado.\n",
    "\n",
    "### 5. (10 pts)  \n",
    "Menciona dos ventajas y dos desventajas del Random Forest comparado con un solo árbol. Sé específico, no generalices.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Disminuye la varianza del modelo, ya que suaviza el efecto de outliers y errores individuales presentes en los datos.\n",
    "\n",
    "- Reduce el riesgo de overfitting al usar bootstrap cada árbol se entrena con una muestra distinta del dataset, lo que introduce variabilidad y mejora la capacidad de generalización del modelo.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "- Tiene un alto costo computacional, ya que implica entrenar y almacenar decenas o cientos de árboles, además de realizar múltiples procesos de muestreo (bootstrap).\n",
    "\n",
    "- Menor interpretabilida al ser muchos árboles, resulta difícil visualizar cómo influyen específicamente las variables en cada decisión.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Sección III: Gradient Boosting (25 puntos)\n",
    "\n",
    "### 6. (10 pts)  \n",
    "Explica, paso a paso, cómo funciona el algoritmo de **Gradient Boosting**. Incluye el concepto de residuales y cómo se minimiza la pérdida en cada iteración.\n",
    "\n",
    "Gradient Boosting es un modelo que entrena varios árboles uno tras otro, y cada uno se enfoca en mejorar los errores del anterior. Todo comienza con una predicción inicial. Luego se calculan los residuales, que son las diferencias entre lo que predice el modelo y el valor real. El siguiente árbol se entrena para corregir esos errores. La predicción de este árbol se agrega a la del modelo anterior, pero no por completo, sino con un pequeño ajuste para no exagerar los cambios. Este proceso se repite iteradas veces, corrigiendo poco a poco, hasta que el modelo final logra buenas predicciones.\n",
    "\n",
    "\n",
    "\n",
    "### 7. (15 pts)  \n",
    "¿Cuál es la diferencia entre Gradient Boosting y Random Forest en términos de cómo combinan los árboles? \n",
    "\n",
    "\n",
    "Su diferencia más significativa es que Random Forest busca reducir la varianza promediando muchos árboles entrenados de forma independiente, mientras que Gradient Boosting busca reducir el sesgo corrigiendo iteradamente los errores del modelo anterior.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Sección IV: XGBoost (20 puntos)\n",
    "\n",
    "### 8. (10 pts)  \n",
    "Explica cómo XGBoost optimiza el proceso de boosting usando una expansión de Taylor de segundo orden.\n",
    "\n",
    "Al momento de calcularlo nos damos cuenta que es una función complicada, asi que necesitamos aproximarla a un punto usando la expansión de taylor de segundo orden. \n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2\n",
    "$$\n",
    "\n",
    "Podemos aplicar el gradiente y el hessiano para poder hacer que la función de perdida se optimice en cada iteración.\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 9. (5 pts)  \n",
    "¿Qué es el *similarity score*? ¿Qué es el *output value*? ¿De dónde salen estas fórmulas y cuál es su interpretación?\n",
    "\n",
    "\n",
    "*Similarity score* es una medida que nos indica y nos ayuda a saber qué tan buena es la partición que se está haciendo en el árbol de decisión. Mientras mayor sea el resultado, mejor va a ser la separación que habrá en los datos de la hoja.\n",
    "\n",
    "$$\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}$$\n",
    "\n",
    "*Output value* es el valor que se le da a una hoja terminal en un árbol de decisión dentro del modelo de XGBoost. Esto se puede interpretar como el valor que toma cada hoja para minimizar la pérdida en esa región. Esta fórmula proviene de haber aplicado una aproximación de segundo orden de Taylor a la función de pérdida.\n",
    "\n",
    "$$\n",
    "\\text{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "\n",
    "### 10. (5 pts)  \n",
    "XGBoost y otros modelos de gradient boosting permiten evaluar la importancia de las variables con diferentes métricas: `weight` y `gain`. Explica qué representa cada una. ¿Cuál crees que es más útil para interpretar un modelo y por qué?\n",
    "\n",
    "\n",
    "`Weight` es el peso que puede tener una variable en el modelo, es decir, representa las veces que la variable fue utilizada para hacer una partición. Si el valor de weight es alto, significa que la variable fue considerada muchas veces como punto de división en los árboles, lo cual indica que tuvo cierta relevancia en la construcción del modelo.\n",
    "\n",
    "`Gain` es la ganancia, o mejora promedio, en la función de pérdida cuando se usa esa variable para dividir. Esta mide qué tanto ayudó esa variable a reducir el error del modelo cada vez que se utilizó. Si la ganancia es positiva , significa que la pérdida disminuyó.\n",
    "\n",
    "Creo que gain suele ser más útil para interpretar un modelo, porque no solo cuenta cuántas veces se usó una variable, sino qué tanto ayudó a mejorar el modelo. Una variable puede tener pocas divisiones pero ser muy efectiva, lo cual se refleja mejor en gain que en weight.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Sección V: LightGBM y CatBoost (15 puntos)\n",
    "\n",
    "### 10. (5 pts)  \n",
    "Explica qué es **histogram-based splitting** y cómo lo implementa LightGBM para ganar velocidad.\n",
    "\n",
    "LightGBM lo implementa donde en lugar de probar todos los valores posibles para hacer una división, agrupa los datos en intervalos (bins) y solo busca el mejor corte entre esos grupos. Esto hace que el proceso sea mucho más rápido y ligero computacionalmente hablando, ya que trabaja con enteros en lugar de valores continuos.\n",
    "\n",
    "\n",
    "\n",
    "### 11. (5 pts)  \n",
    "¿Qué problema específico resuelve CatBoost respecto al manejo de variables categóricas? ¿Cómo lo hace?\n",
    "\n",
    "Que en catboost las variables categóricas no necesita codificacion manual. Usa un tipo de target encoding especial, llamado ordered target encoding, que está regularizado y diseñado específicamente para evitar tanto el overfitting como el data leakage, esto es muy util con respecto a los otros modelos como lightgbm y xgbost\n",
    "\n",
    "\n",
    "### 12. (5 pts)  \n",
    "Compara LightGBM y CatBoost: ¿cuándo usarías uno sobre el otro? Sé claro y justifica en base a tipo de datos, velocidad o precisión.\n",
    "\n",
    "Si tengo muchas variables categóricas, probablemente usaría CatBoost, ya que maneja estas variables de forma automática y evita el target leakage con técnicas como el ordered boosting. Además, su documentación es bastante clara y enfocada en evitar errores comunes con datos categóricos.\n",
    "\n",
    "Por otro lado, si trabajo principalmente con variables numéricas o necesito resultados rápidos (por ejemplo, si trabajo en una oficina y me piden un reporte rapido ), preferiría LightGBM. Este modelo es más ligero, rápido, con mejor soporte para GPU, y requiere menos memoria. Eso sí, tiene dos limitantes: no maneja directamente variables categóricas y su documentación no es tan completa como la de CatBoost.\n",
    "\n",
    "CatBoost puede ser más preciso en tareas complejas, pero también es más sensible al orden de los datos y puede tardar más en entrenar. En resumen, si busco velocidad y eficiencia, elijo LightGBM; si busco precisión con variables categóricas, elijo CatBoost.\n",
    "\n",
    "\n",
    "\n",
    "## Sección VI: Power Analysis (5 puntos)\n",
    "\n",
    "### 13. (5 pts)  \n",
    "¿Qué es un *power analysis* y para qué sirve? ¿En qué contexto lo hemos usado en clase y por qué es importante antes de correr un experimento?\n",
    "\n",
    "Un power analysis es un análisis que se hace antes de un experimento para determinar cuántas observaciones o muestras se necesitan para detectar un efecto si realmente existe.\n",
    "\n",
    "En clase lo hemos usado en el primer parcial con las pruebas A/B, para asegurarnos de que nuestros resultados fueran estadísticamente buenos. Es importante hacerlo antes del experimento porque ayuda a planear bien el tamaño de muestra o evitar que no haya los datos suficientes al realizarlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782e43b-cda4-43f3-998c-f3f535111a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
