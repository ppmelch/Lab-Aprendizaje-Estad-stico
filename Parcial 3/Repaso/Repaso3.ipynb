{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaso $3er $ Parcial \n",
    "### Laboratorio de Aprendizaje Estadístico\n",
    "José Armando Melchor Soto\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temario\n",
    "\n",
    "1. Arboles de decisión \n",
    "    - Regresión \n",
    "    - Clasificación\n",
    "2. Random Forest\n",
    "\n",
    "3. Gradient boosting\n",
    "    - xgboost\n",
    "    - lightgbm\n",
    "    - catboost\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Qué son?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un árbol de decisión es un modelo predictivo que organiza los datos en distintos niveles jerárquicos, realizando divisiones sucesivas según ciertos valores límite. En cada nivel, separa los datos dependiendo de si cumplen o no con una condición específica, agrupándolos según esas decisiones para llegar a una predicción final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo recorre todas las características y todos los posibles umbrales de división. Luego, selecciona el umbral que cause la mayor reducción de varianza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Reducción de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador considera todas las variables y posibles puntos de corte, pero en este caso selecciona aquel que mejor separa las clases, buscando el umbral que maximice la ganancia de información.\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de impureza} = \\text{Impureza nodo padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Qué es?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un algoritmo de machine learning basado en árboles de decisión, que crea muchos árboles y combina sus predicciones para mejorar la precisión y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan varias muestras aleatorias del dataset original usando bootstrap. A partir de esas muestras, se entrenan múltiples árboles. Luego, cada uno hace su predicción y, al final, se toma el promedio de todas las predicciones para obtener el resultado final del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenan varios árboles con diferentes muestras de datos. Pero en lugar de promediar, se toma la clase que más veces fue predicha por los árboles, es decir, se elige la modal entre todas las predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Qué es?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting es un método en machine learning que entrena varios modelos uno tras otro, buscando que cada nuevo modelo corrija los errores del anterior. Hay versiones mejoradas de este enfoque, como XGBoost, LightGBM y CatBoost, que lo hacen más rápido y eficiente.\n",
    "\n",
    "En estos modelos, la predicción de cada árbol suele ser el promedio de los valores de sus hojas. Y la diferencia entre un regresor y un clasificador es que en el clasificador se usan log-odds para hacer la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se calcula XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un modelo que se construye al sumar un nuevo árbol iterativamente a la predicción. \n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Donde el objetivo de cada interación , es que en el nuevo árbol $f_t$ se minimice la contribución marginal a la función de pérdida total.\n",
    "Esta función objetivo consiste en dos partes: \n",
    "- La primera parte es la ya construida:\n",
    "$$\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "- La segunda parte es en la que se añade un nuevo árbol $f_t$\n",
    "$$\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Quedando como :\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "}_{\\text{Parte ya construida (constante en esta iteración)}} +\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "}_{\\text{Lo que añade el nuevo árbol $f_t$}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde $\\Omega$ es la regurizacón de la penalización de los árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al momento de calcularlo nos damos cuenta que es una función complicada, asi que necesitamos aproximarla a un punto usando la expansión de taylor de segundo orden. \n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aplicar el gradiente y el hessiano para poder hacer que la función de perdida se optimice en cada iteración.\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asigna un valor constante $w_j$ a cada región $R_j$  , esto hace que se pueda regularizar la función de perdida.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica para cada hoja , ya que anteriormente se minimizó una aproximación cuadrática local de la pérdida.\n",
    "$$\n",
    "\\text{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este valor representa el doble de la reducción en la pérdida que se logra con el outputvalue , anteriormente calculado.\n",
    "\n",
    "$$\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el valor anterior, podemos decidir si conviene dividir una hoja en un árbol de decisión, se calcula la ganancia del split, que mide la mejora en la calidad del modelo después del split. Si la ganancia es positiva, significa que la pérdida disminuyó.\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya para la predicción final, esta se obtiene sumando todos los árboles.\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM implementa el algoritmo de Gradient Boosting, que entrena múltiples árboles secuenciales donde cada uno corrige los errores del anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost es un algoritmo de machine learning basado en gradient boosting, que destaca por manejar variables categóricas automáticamente y ofrecer alta precisión en tareas de clasificación y regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa: XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "| Característica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | Rápido, pero más lento que LightGBM y CatBoost             | 🔥 Muy rápido gracias a histogramas y leaf-wise growth      | Rápido, aunque un poco más lento que LightGBM                  |\n",
    "| **Precisión**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categóricas                        |\n",
    "| **Variables categóricas**   | ❌ No las maneja (requiere encoding manual)                | ❌ No las maneja (requiere encoding manual)                 | ✅ Soporte nativo + regularización secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ✅ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ✅ Automático                                               | ✅ Automático                                                | ✅ Automático                                                   |\n",
    "| **Soporte GPU**             | ✅ Sí (bastante estable)                                   | ✅ Sí (muy rápido)                                           | ✅ Sí (algo más limitado)                                      |\n",
    "| **Instalación**             | Fácil (`pip install xgboost`)                             | Fácil (`pip install lightgbm`)                              | Un poco más pesada (`pip install catboost`)                   |\n",
    "| **Documentación**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacción con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ✅ Neutral                                                  | ✅ Neutral                                                   | ⚠️ Sensible (por codificación secuencial)                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ ¿Cuándo usar cada uno?\n",
    "\n",
    "## ✅ ¿Cuándo usar XGBoost, LightGBM o CatBoost?\n",
    "\n",
    "| Situación                                                  | Recomendación                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular pequeño o mediano                          | ✅ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables numéricas                 | ✅ LightGBM                                         |\n",
    "| Muchas variables categóricas sin preprocesamiento          | ✅ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ✅ XGBoost (muy probado en producción y Kaggle)     |\n",
    "| Entrenamiento rápido con buen desempeño                    | ✅ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ✅ Cualquiera, pero CatBoost da mejores resultados con categóricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ✅ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ✅ XGBoost o LightGBM                               |\n",
    "| Tuning automático (Optuna, GridSearchCV, etc.)             | ✅ LightGBM (rápido y convergente)                  |\n",
    "| Producción en sistemas legacy o APIs bien documentadas     | ✅ XGBoost (mayor madurez, más integración)         |\n",
    "| Clasificación multi-label o problemas no estándar          | ✅ XGBoost (soporte más flexible)                   |\n",
    "\n",
    "\n",
    "## 🧠 Tips\n",
    "\n",
    "- **LightGBM** puede overfittear fácilmente → cuida `num_leaves` y `min_data_in_leaf`.\n",
    "- **CatBoost** funciona muy bien con defaults y sin preprocessing.\n",
    "- **XGBoost** es muy robusto y balanceado, ideal si ya tienes un pipeline con encoding hecho.\n",
    "\n",
    "\n",
    "\n",
    "La estructura del árbol:\n",
    "\n",
    "* XGBoost produce árboles más simétricos y balanceados.\n",
    "\n",
    "* LightGBM produce árboles más profundos y desbalanceados si no se controla.\n",
    "\n",
    "La precisión y riesgo de overfitting:\n",
    "\n",
    "* Leaf-wise (LightGBM) puede encontrar mejores divisiones, pero se sobreajusta más fácil.\n",
    "\n",
    "* Level-wise (XGBoost) es más estable, pero a veces menos preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREGUNTA EXAMEN \n",
    "¿Qué es partial dependence?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una técnica que muestra el efecto promedio que tiene una o más variables independientes sobre la predicción de un modelo, al mantener constantes (o promediar) las demás variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo:\n",
    "\n",
    "Partial dependence sirve para ver cómo una variable (como las habitaciones de una casa) influye en la predicción del modelo. Si queremos ver cómo cambia el precio de una casa cuando aumentamos las habitaciones, usamos partial dependence para calcular el promedio de los precios predichos por el modelo al ir variando solo las habitaciones y manteniendo el resto igual. Así vemos cómo afecta esa variable específica a la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase\n",
    "Un Partial Dependence Plot (PDP) muestra la relación entre una o más características y la predicción de un modelo, manteniendo las demás variables fijas. Es útil para entender cómo un modelo como un Random Forest toma decisiones.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(best_forest, X_train, [7], ax=ax, feature_names=X_train.keys())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
