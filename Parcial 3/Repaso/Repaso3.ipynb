{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaso $3er $ Parcial \n",
    "### Laboratorio de Aprendizaje Estad√≠stico\n",
    "Jos√© Armando Melchor Soto\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temario\n",
    "\n",
    "1. Arboles de decisi√≥n \n",
    "    - Regresi√≥n \n",
    "    - Clasificaci√≥n\n",
    "2. Random Forest\n",
    "\n",
    "3. Gradient boosting\n",
    "    - xgboost\n",
    "    - lightgbm\n",
    "    - catboost\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arboles de decisi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¬øQu√© son?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un √°rbol de decisi√≥n es un modelo predictivo que organiza los datos en distintos niveles jer√°rquicos, realizando divisiones sucesivas seg√∫n ciertos valores l√≠mite. En cada nivel, separa los datos dependiendo de si cumplen o no con una condici√≥n espec√≠fica, agrup√°ndolos seg√∫n esas decisiones para llegar a una predicci√≥n final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresi√≥n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo recorre todas las caracter√≠sticas y todos los posibles umbrales de divisi√≥n. Luego, selecciona el umbral que cause la mayor reducci√≥n de varianza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Reducci√≥n de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El clasificador considera todas las variables y posibles puntos de corte, pero en este caso selecciona aquel que mejor separa las clases, buscando el umbral que maximice la ganancia de informaci√≥n.\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de impureza} = \\text{Impureza nodo padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¬øQu√© es?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un algoritmo de machine learning basado en √°rboles de decisi√≥n, que crea muchos √°rboles y combina sus predicciones para mejorar la precisi√≥n y evitar el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresi√≥n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan varias muestras aleatorias del dataset original usando bootstrap. A partir de esas muestras, se entrenan m√∫ltiples √°rboles. Luego, cada uno hace su predicci√≥n y, al final, se toma el promedio de todas las predicciones para obtener el resultado final del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenan varios √°rboles con diferentes muestras de datos. Pero en lugar de promediar, se toma la clase que m√°s veces fue predicha por los √°rboles, es decir, se elige la modal entre todas las predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¬øQu√© es?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting es un m√©todo en machine learning que entrena varios modelos uno tras otro, buscando que cada nuevo modelo corrija los errores del anterior. Hay versiones mejoradas de este enfoque, como XGBoost, LightGBM y CatBoost, que lo hacen m√°s r√°pido y eficiente.\n",
    "\n",
    "En estos modelos, la predicci√≥n de cada √°rbol suele ser el promedio de los valores de sus hojas. Y la diferencia entre un regresor y un clasificador es que en el clasificador se usan log-odds para hacer la predicci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øC√≥mo se calcula XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un modelo que se construye al sumar un nuevo √°rbol iterativamente a la predicci√≥n. \n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Donde el objetivo de cada interaci√≥n , es que en el nuevo √°rbol $f_t$ se minimice la contribuci√≥n marginal a la funci√≥n de p√©rdida total.\n",
    "Esta funci√≥n objetivo consiste en dos partes: \n",
    "- La primera parte es la ya construida:\n",
    "$$\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "- La segunda parte es en la que se a√±ade un nuevo √°rbol $f_t$\n",
    "$$\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Quedando como :\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "}_{\\text{Parte ya construida (constante en esta iteraci√≥n)}} +\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "}_{\\text{Lo que a√±ade el nuevo √°rbol $f_t$}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde $\\Omega$ es la regurizac√≥n de la penalizaci√≥n de los √°rboles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al momento de calcularlo nos damos cuenta que es una funci√≥n complicada, asi que necesitamos aproximarla a un punto usando la expansi√≥n de taylor de segundo orden. \n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aplicar el gradiente y el hessiano para poder hacer que la funci√≥n de perdida se optimice en cada iteraci√≥n.\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se asigna un valor constante $w_j$ a cada regi√≥n $R_j$  , esto hace que se pueda regularizar la funci√≥n de perdida.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica para cada hoja , ya que anteriormente se minimiz√≥ una aproximaci√≥n cuadr√°tica local de la p√©rdida.\n",
    "$$\n",
    "\\text{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este valor representa el doble de la reducci√≥n en la p√©rdida que se logra con el outputvalue , anteriormente calculado.\n",
    "\n",
    "$$\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el valor anterior, podemos decidir si conviene dividir una hoja en un √°rbol de decisi√≥n, se calcula la ganancia del split, que mide la mejora en la calidad del modelo despu√©s del split. Si la ganancia es positiva, significa que la p√©rdida disminuy√≥.\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya para la predicci√≥n final, esta se obtiene sumando todos los √°rboles.\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM implementa el algoritmo de Gradient Boosting, que entrena m√∫ltiples √°rboles secuenciales donde cada uno corrige los errores del anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost es un algoritmo de machine learning basado en gradient boosting, que destaca por manejar variables categ√≥ricas autom√°ticamente y ofrecer alta precisi√≥n en tareas de clasificaci√≥n y regresi√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa: XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "| Caracter√≠stica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | R√°pido, pero m√°s lento que LightGBM y CatBoost             | üî• Muy r√°pido gracias a histogramas y leaf-wise growth      | R√°pido, aunque un poco m√°s lento que LightGBM                  |\n",
    "| **Precisi√≥n**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categ√≥ricas                        |\n",
    "| **Variables categ√≥ricas**   | ‚ùå No las maneja (requiere encoding manual)                | ‚ùå No las maneja (requiere encoding manual)                 | ‚úÖ Soporte nativo + regularizaci√≥n secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ‚úÖ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ‚úÖ Autom√°tico                                               | ‚úÖ Autom√°tico                                                | ‚úÖ Autom√°tico                                                   |\n",
    "| **Soporte GPU**             | ‚úÖ S√≠ (bastante estable)                                   | ‚úÖ S√≠ (muy r√°pido)                                           | ‚úÖ S√≠ (algo m√°s limitado)                                      |\n",
    "| **Instalaci√≥n**             | F√°cil (`pip install xgboost`)                             | F√°cil (`pip install lightgbm`)                              | Un poco m√°s pesada (`pip install catboost`)                   |\n",
    "| **Documentaci√≥n**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacci√≥n con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ‚úÖ Neutral                                                  | ‚úÖ Neutral                                                   | ‚ö†Ô∏è Sensible (por codificaci√≥n secuencial)                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ ¬øCu√°ndo usar cada uno?\n",
    "\n",
    "## ‚úÖ ¬øCu√°ndo usar XGBoost, LightGBM o CatBoost?\n",
    "\n",
    "| Situaci√≥n                                                  | Recomendaci√≥n                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular peque√±o o mediano                          | ‚úÖ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables num√©ricas                 | ‚úÖ LightGBM                                         |\n",
    "| Muchas variables categ√≥ricas sin preprocesamiento          | ‚úÖ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ‚úÖ XGBoost (muy probado en producci√≥n y Kaggle)     |\n",
    "| Entrenamiento r√°pido con buen desempe√±o                    | ‚úÖ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ‚úÖ Cualquiera, pero CatBoost da mejores resultados con categ√≥ricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ‚úÖ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ‚úÖ XGBoost o LightGBM                               |\n",
    "| Tuning autom√°tico (Optuna, GridSearchCV, etc.)             | ‚úÖ LightGBM (r√°pido y convergente)                  |\n",
    "| Producci√≥n en sistemas legacy o APIs bien documentadas     | ‚úÖ XGBoost (mayor madurez, m√°s integraci√≥n)         |\n",
    "| Clasificaci√≥n multi-label o problemas no est√°ndar          | ‚úÖ XGBoost (soporte m√°s flexible)                   |\n",
    "\n",
    "\n",
    "## üß† Tips\n",
    "\n",
    "- **LightGBM** puede overfittear f√°cilmente ‚Üí cuida `num_leaves` y `min_data_in_leaf`.\n",
    "- **CatBoost** funciona muy bien con defaults y sin preprocessing.\n",
    "- **XGBoost** es muy robusto y balanceado, ideal si ya tienes un pipeline con encoding hecho.\n",
    "\n",
    "\n",
    "\n",
    "La estructura del √°rbol:\n",
    "\n",
    "* XGBoost produce √°rboles m√°s sim√©tricos y balanceados.\n",
    "\n",
    "* LightGBM produce √°rboles m√°s profundos y desbalanceados si no se controla.\n",
    "\n",
    "La precisi√≥n y riesgo de overfitting:\n",
    "\n",
    "* Leaf-wise (LightGBM) puede encontrar mejores divisiones, pero se sobreajusta m√°s f√°cil.\n",
    "\n",
    "* Level-wise (XGBoost) es m√°s estable, pero a veces menos preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREGUNTA EXAMEN \n",
    "¬øQu√© es partial dependence?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una t√©cnica que muestra el efecto promedio que tiene una o m√°s variables independientes sobre la predicci√≥n de un modelo, al mantener constantes (o promediar) las dem√°s variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo:\n",
    "\n",
    "Partial dependence sirve para ver c√≥mo una variable (como las habitaciones de una casa) influye en la predicci√≥n del modelo. Si queremos ver c√≥mo cambia el precio de una casa cuando aumentamos las habitaciones, usamos partial dependence para calcular el promedio de los precios predichos por el modelo al ir variando solo las habitaciones y manteniendo el resto igual. As√≠ vemos c√≥mo afecta esa variable espec√≠fica a la predicci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase\n",
    "Un Partial Dependence Plot (PDP) muestra la relaci√≥n entre una o m√°s caracter√≠sticas y la predicci√≥n de un modelo, manteniendo las dem√°s variables fijas. Es √∫til para entender c√≥mo un modelo como un Random Forest toma decisiones.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(best_forest, X_train, [7], ax=ax, feature_names=X_train.keys())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
