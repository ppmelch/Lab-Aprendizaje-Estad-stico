{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regresión Softmax: Explicación Teórica**\n",
    "\n",
    "La **regresión Softmax** es una generalización de la regresión logística para problemas de clasificación multiclase. Mientras que la regresión logística se usa para clasificar entre dos clases (0 y 1), la regresión Softmax permite clasificar entre múltiples clases modelando probabilidades para cada una de ellas.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Origen: Generalización de la Regresión Logística**\n",
    "\n",
    "En regresión logística binaria, modelamos la probabilidad de que una instancia $\\mathbf{x}$ pertenezca a la clase $y = 1$ usando la función sigmoide:\n",
    "\n",
    "$$\n",
    "P(y = 1 | \\mathbf{x}) = \\frac{1}{1 + \\exp(- (\\mathbf{w} \\cdot \\mathbf{x} + b))}\n",
    "$$\n",
    "\n",
    "Para clasificación multiclase, necesitamos una función que asigne probabilidades a **múltiples clases** en lugar de solo una. Esto se logra con la **función Softmax**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Definición de la Función Softmax**\n",
    "\n",
    "La función Softmax convierte los logits (salidas de la función lineal) en probabilidades:\n",
    "\n",
    "$$\n",
    "P(y = k | \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_k \\cdot \\mathbf{x} + b_k)}{\\sum_{j=1}^{K} \\exp(\\mathbf{w}_j \\cdot \\mathbf{x} + b_j)}\n",
    "$$\n",
    "\n",
    "### **¿Qué significa esta ecuación?**\n",
    "- Para cada clase $k$, calculamos un **logit** con su propio vector de pesos $\\mathbf{w}_k$ y bias $b_k$.\n",
    "- Aplicamos la función exponencial para mantener las probabilidades **siempre positivas**.\n",
    "- Normalizamos dividiendo entre la suma de todas las exponenciales, asegurando que las probabilidades sumen 1.\n",
    "\n",
    "#### **Ejemplo Numérico**\n",
    "Si tenemos 3 clases y logits calculados como:\n",
    "$$\n",
    "z_1 = 2.0, \\quad z_2 = 1.0, \\quad z_3 = -1.0\n",
    "$$\n",
    "Aplicamos Softmax:\n",
    "$$\n",
    "P(y=1) = \\frac{e^2}{e^2 + e^1 + e^{-1}} = 0.72\n",
    "$$\n",
    "$$\n",
    "P(y=2) = \\frac{e^1}{e^2 + e^1 + e^{-1}} = 0.26\n",
    "$$\n",
    "$$\n",
    "P(y=3) = \\frac{e^{-1}}{e^2 + e^1 + e^{-1}} = 0.04\n",
    "$$\n",
    "Esto indica que la clase 1 es la más probable.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Función de Pérdida: Entropía Cruzada**\n",
    "\n",
    "Para entrenar el modelo, usamos la **entropía cruzada**:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} \\mathbb{1}(y_i = k) \\log P(y_i = k | \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "### **¿Qué significa esta ecuación?**\n",
    "- Es una suma sobre todas las muestras ($N$) y todas las clases ($K$).\n",
    "- $\\mathbb{1}(y_i = k)$ es 1 si la clase correcta es $k$, y 0 en caso contrario.\n",
    "- Penaliza predicciones erróneas con mayor intensidad.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Gradiente Descendente: Cómo se Ajustan los Pesos**\n",
    "\n",
    "Para minimizar la pérdida, calculamos el gradiente de $L$ respecto a los pesos de la clase $k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_k} = \\sum_{i=1}^{N} \\left(P(y_i = k | \\mathbf{x}_i) - \\mathbb{1}(y_i = k)\\right) \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "**Actualización de pesos con descenso en gradiente:**\n",
    "$$\n",
    "\\mathbf{w}_k \\leftarrow \\mathbf{w}_k - \\eta \\frac{\\partial L}{\\partial \\mathbf{w}_k}\n",
    "$$\n",
    "\n",
    "Donde $\\eta$ es la **tasa de aprendizaje**.\n",
    "\n",
    "### **¿Cómo interpretamos el gradiente?**\n",
    "- Si la predicción $P(y_i = k)$ es **mayor** que la etiqueta real, reducimos $\\mathbf{w}_k$.\n",
    "- Si es **menor**, aumentamos $\\mathbf{w}_k$.\n",
    "\n",
    "Esto ajusta el modelo para mejorar la clasificación en cada iteración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# One-hot encoding de las etiquetas\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Dividir en entrenamiento y prueba\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "c\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n",
    "\n",
    "# One-hot encoding de las etiquetas\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicialización de pesos y bias\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(X.shape[1], y_one_hot.shape[1]) * 0.01\n",
    "b = np.zeros((1, y_one_hot.shape[1]))\n",
    "\n",
    "# Función Softmax\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Parámetros del entrenamiento\n",
    "lr = 0.1\n",
    "num_epochs = 500\n",
    "loss_history = []\n",
    "\n",
    "# Gradiente Descendente\n",
    "for epoch in range(num_epochs):\n",
    "    logits = np.dot(X_train, W) + b\n",
    "    y_pred = softmax(logits)\n",
    "    \n",
    "    dW = np.dot(X_train.T, (y_pred - y_train)) / len(X_train)\n",
    "    db = np.sum(y_pred - y_train, axis=0, keepdims=True) / len(X_train)\n",
    "    \n",
    "    W -= lr * dW\n",
    "    b -= lr * db\n",
    "    \n",
    "    loss = -np.sum(y_train * np.log(y_pred + 1e-9)) / len(X_train)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Época {epoch}, Pérdida: {loss:.4f}\")\n",
    "\n",
    "# Gráfica de la función de pérdida\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.title(\"Evolución de la pérdida en Wine Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora con sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppmel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ppmel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "\n",
    "# Cargar el dataset Wine\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Crear un DataFrame con nombres de columnas\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df[\"target\"] = y  # Agregar la columna de la clase\n",
    "\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "X = df.drop(target, axis=1)\n",
    "\n",
    "# Normalizar las características\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "# Crear y entrenar el modelo de Regresión Logística Multiclase (Softmax)\n",
    "model = linear_model.LogisticRegression(multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550561797752809"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=predicciones, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de coeficientes\n",
    "coeficientes = pd.DataFrame({'variables': X_train.keys(),\n",
    "                            'thetas0': model.coef_[0],\n",
    "                            'thetas1': model.coef_[1],\n",
    "                            'thetas2': model.coef_[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.35408296e-01, -1.49204751e-02,  1.18566719e-01,\n",
       "        -2.28181754e-01,  1.55565359e-02,  2.29335034e-01,\n",
       "         4.03904382e-01, -1.82958972e-02,  4.51303208e-03,\n",
       "        -1.28707826e-01,  1.44853030e-02,  2.06817176e-01,\n",
       "         6.85629521e-03],\n",
       "       [ 4.00840309e-01, -5.85380771e-01, -1.36947547e-01,\n",
       "         1.88109151e-03,  5.35481171e-02,  1.54185498e-01,\n",
       "         3.52243846e-01, -1.58022517e-02,  3.09327849e-01,\n",
       "        -1.23089564e+00,  1.92868192e-01,  4.33285700e-01,\n",
       "        -7.91951642e-03],\n",
       "       [-1.65432014e-01,  6.00301246e-01,  1.83808276e-02,\n",
       "         2.26300663e-01, -6.91046529e-02, -3.83520532e-01,\n",
       "        -7.56148228e-01,  3.40981489e-02, -3.13840881e-01,\n",
       "         1.35960347e+00, -2.07353495e-01, -6.40102876e-01,\n",
       "         1.06322121e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>thetas0</th>\n",
       "      <th>thetas1</th>\n",
       "      <th>thetas2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>-0.235408</td>\n",
       "      <td>0.400840</td>\n",
       "      <td>-0.165432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malic_acid</td>\n",
       "      <td>-0.014920</td>\n",
       "      <td>-0.585381</td>\n",
       "      <td>0.600301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ash</td>\n",
       "      <td>0.118567</td>\n",
       "      <td>-0.136948</td>\n",
       "      <td>0.018381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alcalinity_of_ash</td>\n",
       "      <td>-0.228182</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.226301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>magnesium</td>\n",
       "      <td>0.015557</td>\n",
       "      <td>0.053548</td>\n",
       "      <td>-0.069105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_phenols</td>\n",
       "      <td>0.229335</td>\n",
       "      <td>0.154185</td>\n",
       "      <td>-0.383521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flavanoids</td>\n",
       "      <td>0.403904</td>\n",
       "      <td>0.352244</td>\n",
       "      <td>-0.756148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nonflavanoid_phenols</td>\n",
       "      <td>-0.018296</td>\n",
       "      <td>-0.015802</td>\n",
       "      <td>0.034098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>proanthocyanins</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.309328</td>\n",
       "      <td>-0.313841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>color_intensity</td>\n",
       "      <td>-0.128708</td>\n",
       "      <td>-1.230896</td>\n",
       "      <td>1.359603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hue</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>0.192868</td>\n",
       "      <td>-0.207353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>od280/od315_of_diluted_wines</td>\n",
       "      <td>0.206817</td>\n",
       "      <td>0.433286</td>\n",
       "      <td>-0.640103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>proline</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>-0.007920</td>\n",
       "      <td>0.001063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       variables   thetas0   thetas1   thetas2\n",
       "0                        alcohol -0.235408  0.400840 -0.165432\n",
       "1                     malic_acid -0.014920 -0.585381  0.600301\n",
       "2                            ash  0.118567 -0.136948  0.018381\n",
       "3              alcalinity_of_ash -0.228182  0.001881  0.226301\n",
       "4                      magnesium  0.015557  0.053548 -0.069105\n",
       "5                  total_phenols  0.229335  0.154185 -0.383521\n",
       "6                     flavanoids  0.403904  0.352244 -0.756148\n",
       "7           nonflavanoid_phenols -0.018296 -0.015802  0.034098\n",
       "8                proanthocyanins  0.004513  0.309328 -0.313841\n",
       "9                color_intensity -0.128708 -1.230896  1.359603\n",
       "10                           hue  0.014485  0.192868 -0.207353\n",
       "11  od280/od315_of_diluted_wines  0.206817  0.433286 -0.640103\n",
       "12                       proline  0.006856 -0.007920  0.001063"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      4.574868\n",
       "1      5.158047\n",
       "2      4.493531\n",
       "3      6.937278\n",
       "4      1.023761\n",
       "         ...   \n",
       "173   -1.181487\n",
       "174   -1.369570\n",
       "175   -0.270735\n",
       "176   -0.037876\n",
       "177   -3.430585\n",
       "Length: 178, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z0=(model.coef_[0]* X).sum(axis=1)+model.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1=(model.coef_[1]* X).sum(axis=1)+model.intercept_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=(model.coef_[2]* X).sum(axis=1)+model.intercept_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9895481314687268)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(z0[0])/(np.exp(z0[0])+np.exp(z1[0])+np.exp(z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.010348238068246168)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(z1[0])/(np.exp(z0[0])+np.exp(z1[0])+np.exp(z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.00010363046302689827)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(z2[0])/(np.exp(z0[0])+np.exp(z1[0])+np.exp(z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
