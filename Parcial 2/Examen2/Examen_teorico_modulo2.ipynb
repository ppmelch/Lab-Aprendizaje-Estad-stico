{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58182883-edf4-410e-9fb5-3cc408d49dfb",
   "metadata": {},
   "source": [
    "# Examen modulo 2\n",
    "\n",
    "\n",
    "José Armando Melchor Soto\n",
    "---\n",
    "\n",
    "## **Sección 1: Regresión Logística** (30 puntos)  \n",
    "---\n",
    "1. **(10 pts)** Explica la diferencia entre la regresión logística **lineal** y la **polinomial**. ¿En qué casos es recomendable usar la versión polinomial?  \n",
    "\n",
    "La regresión logística lineal utiliza una combinación lineal de las variables para predecir la probabilidad de una clase, funcionando bien cuando las clases pueden separarse con una línea recta o un plano. Es un modelo sencillo y eficiente cuando la relación entre las variables es directa y lineal Mientras que la regresión logística polinomial incorpora términos polinómicos de las variables para capturar relaciones más complejas, permitiendo que las fronteras de decisión sean curvas. Este modelo es útil cuando la separación entre las clases no es lineal, y la regresión logística lineal no puede proporcionar un ajuste adecuado. Aunque más potente en cuanto a flexibilidad, la regresión logística polinomial tiene un mayor costo computacional, lo que implica que el creador del modelo debe evaluar si los beneficios en precisión justifican este costo adicional.\n",
    "\n",
    "\n",
    "---\n",
    "2. **(10 pts)** Explica como mediante decenso en gradiente y maxima verosimilitud creamos una regresión lógisitca  \n",
    "\n",
    "\n",
    "Se define la función de verosimilitud para los datos, utilizando la distribución de Bernoulli y la función sigmoide:\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{m} p(y_i | X_i; \\theta) = \\prod_{i=1}^{m} \\left[ \\sigma(\\theta^T X_i) \\right]^{y_i} \\cdot \\left[ 1 - \\sigma(\\theta^T X_i) \\right]^{(1 - y_i)}\n",
    "$$\n",
    "\n",
    "Se aplica el logaritmo a la función de verosimilitud para facilitar la optimización, convirtiéndola en una forma que podemos maximizar:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(\\theta^T X_i) + (1 - y_i) \\log (1 - \\sigma(\\theta^T X_i)) \\right]\n",
    "$$\n",
    "\n",
    "A lo anterior tratamos de maximixar el $\\log L(\\theta)$ para poder estimar $(\\theta)$\n",
    "y nos queda como : \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(\\theta^T X_i) + (1 - y_i) \\log (1 - \\sigma(\\theta^T X_i)) \\right]\n",
    "$$\n",
    "\n",
    "Con esto podríamos crear una regresión logística, pero el descenso en gradiente se usa comúnmente para encontrar los valores óptimos de $\\theta$ minimizando la función de pérdida. En el caso de la regresión logística, sin embargo, lo que buscamos es maximizar la función de verosimilitud, ya que, a diferencia de la regresión lineal, la función es creciente. Por lo tanto, en regresión logística necesitamos maximizar la verosimilitud, lo que en términos prácticos se realiza a través del ascenso en gradiente.\n",
    "\n",
    "\n",
    "---\n",
    "3. **(10 pts)** Explica el concepto de **odds** y **log-odds** en regresión logística. ¿Por qué la regresión logística predice el **log-odds** en lugar de la probabilidad directamente? Justifica esto   \n",
    "\n",
    "\n",
    "#### Oods y Log-Odds\n",
    "Los odds son la probabilidad de éxito contra la de fracaso \n",
    "\n",
    "$$odds = \\frac{p}{1-p}$$\n",
    "\n",
    "Log-odds es cuando le aplicas el logaritmo a los odds ya que el crecimiento es exponencial. \n",
    "\n",
    "$$log(odds) = log(\\frac{p}{1-p})$$\n",
    "\n",
    "Una vez se obtienen los coeficientes , se utiliza logit z como: $$z = \\Theta ^T X $$\n",
    "\n",
    "y luego se aplica la función sigmoide para transformar z en una probabilidad.\n",
    "\n",
    "$$\n",
    "p= \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "p se clasifica como uno\n",
    "\n",
    "Se clasifica como :\n",
    "- Si p < 0.5  es 0\n",
    "- Si p > 0.5  es 1\n",
    "\n",
    "Se predice el log-odds porque este facilita el ajuste del modelo y este permite la relación entre las variables y evita las restricciones, ya que el crecimiento de los odds es un crecimiento exponencial.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Sección 2: Área Bajo la Curva (AUC)** (20 puntos)  \n",
    "\n",
    "7. **(5 pts)** Define la **curva ROC** y el AUC. ¿Qué tiene de especial\n",
    "\n",
    "La curva ROC muestra la tasa de verdaderos positivos frente a la tasa de falsos positivos en distintos umbrales de clasificación. Cuanto más arriba y a la izquierda se encuentre la curva, mejor es el modelo para distinguir entre clases, ayudando a la precisión del modelo.\n",
    "\n",
    "El AUC es una métrica que mide el rendimiento de un modelo y corresponde al área bajo la curva ROC. Intuitivamente, representa la probabilidad de que, al comparar dos instancias al azar (una de clase 1 y otra de clase 0), el modelo asigne una mayor probabilidad a la instancia de clase 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8. **(5 pts)** Cuando hacemos una curva ROC, siempre ponemos una diagonal, explica que es esa diagonal  \n",
    "\n",
    "Cuando hacemos una curva ROC , la diagonal de esta refleja el desempeño de un modelo que clasifica aleatoriamente, donde las probabilidades de acertar en cada categoría son iguales. Esto indica que el modelo no tiene capacidad de diferenciar entre la particón de las clases, y su trayectoria es una línea recta sin importar el umbral de decisión.\n",
    "\n",
    "\n",
    "\n",
    "9. **(5 pts)** Un modelo tiene un **AUC de 0.85**. Explica qué significa esto en términos de su capacidad de clasificación.  \n",
    "\n",
    "Significa que el modelo tiene una alta capacidad para distinguir entre clases, con un 85% de probabilidad de que asigne correctamente una muestra positiva sobre una negativa. Sin embargo, aún queda margen para mejorar su rendimiento en la clasificación.\n",
    "\n",
    "\n",
    "10. **(5 pts)** Un modelo tiene accuracy de 99% pero AUC de 0.5%, ¿Cómo es que esto podría suceder?\n",
    "\n",
    "Aunque no es tal cual overfitting, existe una similitud, ya que el modelo tiende a memorizar la clase mayoritaria sin hacer una correcta distinción entre las clases. Esto genera un gran desbalance entre las clases, donde la clase minoritaria se ve mal representada. El AUC bajo refleja esta falta de capacidad para diferenciar entre clases, mostrando que el modelo no tiene un buen poder discriminativo. A pesar de lograr un accuracy alto debido a la predicción de la clase mayoritaria, el modelo no es capaz de clasificar adecuadamente la clase minoritaria, lo que se ve con el AUC de 50%.\n",
    "\n",
    "\n",
    "\n",
    "## **Sección 3: Análisis del Discriminante Lineal (LDA)** (10 puntos)  \n",
    "\n",
    "11. **(10 pts)** ¿Qué es el análisis del discriminante lineal? ¿En que casos lo usarías? (gausiano)\n",
    "\n",
    "Es un modelo de clasificación supervisada utilizado en aprendizaje automático y estadística, que asume que los datos de cada clase siguen una distribución normal. Donde se estiman las probabilidades de pertenencia a cada clase.\n",
    "\n",
    "Distribución normal multivariada : \n",
    "     $$\n",
    "     P(\\mathbf{x} | y = k) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\mu_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k)\\right)\n",
    "     $$\n",
    "\n",
    "Y se utiliza la regla de bayes para calcular la probabilidad posterior\n",
    "     $$\n",
    "     P(y = k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y = k) P(y = k)}{P(\\mathbf{x})}\n",
    "     $$\n",
    "\n",
    "Este modelo lo usaría cuando quiero discriminar entre diferentes grupos de datos, es decir, cuando busco realizar una partición o separación de los datos en clases distintas, permitiendo así clasificar nuevas observaciones en sus respectivos grupos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Sección 4: Cross validation**  (10 puntos)  \n",
    "\n",
    "12. **(10 pts)** ¿Qué es grid search? ¿qué es random search? Explica las diferencias y cuando usarías cada uno \n",
    "\n",
    "Grid search es la búsqueda en la que se trata de probar todas las combinaciones posibles de hiperparámetros en un espacio de búsqueda estructurado. Este encuentra la mejor combinación y solución dentro de los hiperparámetros, sin embargo, no explora valores fuera del grid. Mientras que random search busca lo contrario: en lugar de probar todas las posibles combinaciones, selecciona muchos valores aleatorios dentro de un rango. Este es mucho más rápido que el grid search y, a su vez, permite explorar un espacio más grande de los hiperparámetros, pero su única contra es que no garantiza la mejor combinación. \n",
    "\n",
    "Usaría random search cuando el espacio de hiperparámetros es grande, ya que es más rápido y proporciona un buen resultado, aunque no el mejor. En cambio, grid search lo usaría cuando quiero asegurarme de encontrar la mejor combinación posible dentro de un espacio de hiperparámetros más pequeño.\n",
    "\n",
    "\n",
    "\n",
    "## **Sección 5: Redes Neuronales y Perceptrón Multicapa** (20 puntos)  \n",
    "\n",
    "13. **(5 pts)** Explica que es una red neuronal, que hace, como funciona, etc.   \n",
    "\n",
    "Una red neuronal es otro modelo , este es un modelo de aprendizaje automático formado por neuronas artificiales organizadas en capas de entrada, ocultas y salida. Estas neuronas procesan la información a través de conexiones ponderadas, lo que permite aprender patrones y hacer predicciones a partir de los datos. El funcionamiento de una red neuronal se divide en dos fases principales: Forward Propagation y Backpropagation , que se encargan de calcular las predicciones y ajustar los pesos para mejorar el modelo.\n",
    "\n",
    "\n",
    "14. **(5 pts)** ¿Cuál es el propósito de la **backpropagation** en el entrenamiento de redes neuronales?  \n",
    "\n",
    "El propósito de la backpropagation es corregir los errores de predicción en la red neuronal. El error se propaga hacia atrás a través de la red, distribuyéndose entre las neuronas de cada capa. Luego, se ajustan los pesos y sesgos para minimizar el error, optimizando el proceso de forward propagation y mejorando la precisión del modelo.\n",
    "\n",
    "15. **(5 pts)** A grandes rasgos, explica como obtenemos los coeficientes de una red neuronal  \n",
    "\n",
    "Se obtienen con las dos fases principales anterior mente mencionadas; Forward Propagation y Backpropagation, que se encargan de calcular las predicciones y ajustar los pesos para mejorar el modelo. Estos procesos se repiten de manera iterativa, lo que permite que los pesos converjan a valores óptimos, mejorando así la precisión de las predicciones\n",
    "\n",
    "\n",
    "## **Sección 6: Softmax** (10 puntos)  \n",
    "16. **(5 pts)** Explica que es softmax, para que sirve y como se calcula\n",
    "\n",
    "Es una función que convierte los logits en probabilidades, asegurando que todas sean positivas y sumen uno. Permite clasificar entre múltiples clases al modelar una distribución de probabilidades sobre ellas.\n",
    "\n",
    "Se calcula como : \n",
    "\n",
    "$$\n",
    "P(y = k | \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_k \\cdot \\mathbf{x} + b_k)}{\\sum_{j=1}^{K} \\exp(\\mathbf{w}_j \\cdot \\mathbf{x} + b_j)}\n",
    "$$\n",
    "\n",
    "Donde, para cada $k$, se calcula un logit utilizando el vector de peso $w_k$ y el bias $b_k$. La función exponencial garantiza que los valores sean siempre positivos, y la normalización mediante la suma asegura que todas las probabilidades sumen uno.\n",
    "\n",
    "\n",
    "Ejemplo : \n",
    "\n",
    "Si tenemos 3 clases y logits calculados como:\n",
    "$$\n",
    "z_1 = 2.0, \\quad z_2 = 1.0, \\quad z_3 = -1.0\n",
    "$$\n",
    "Aplicamos Softmax:\n",
    "$$\n",
    "P(y=1) = \\frac{e^2}{e^2 + e^1 + e^{-1}} = 0.72\n",
    "$$\n",
    "$$\n",
    "P(y=2) = \\frac{e^1}{e^2 + e^1 + e^{-1}} = 0.26\n",
    "$$\n",
    "$$\n",
    "P(y=3) = \\frac{e^{-1}}{e^2 + e^1 + e^{-1}} = 0.04\n",
    "$$\n",
    "\n",
    "Esto indica que la clase 1 es la más probable, por lo tanto este modelo se clasificaría como uno .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Puntaje Total: 100 puntos**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e953698",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
